{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def config():\n",
    "    args = EasyDict()\n",
    "    args.train_set = \"/home/hwiric/SagDRE-2024-07-05/data/DocRED/train_annotated.json\"\n",
    "    args.dev_set = \"/home/hwiric/SagDRE-2024-07-05/data/DocRED/dev.json\"\n",
    "    args.test_set = \"/home/hwiric/SagDRE-2024-07-05/data/DocRED/test.json\"\n",
    "\n",
    "    args.checkpoint_dir = \"checkpoint\"\n",
    "    args.model_name = \"SAGDRE_BERT_base\"\n",
    "    args.pretrain_model = \"\"\n",
    "\n",
    "    args.relation_nums = 97\n",
    "    args.entity_type_num = 7\n",
    "    args.max_entity_num = 80\n",
    "\n",
    "    args.word_pad = 0\n",
    "    args.entity_type_pad = 0\n",
    "    args.entity_id_pad = 0\n",
    "\n",
    "    args.word_emb_size = 10\n",
    "    args.use_entity_type = \"store_true\"\n",
    "    args.entity_type_size = 20\n",
    "\n",
    "    args.use_entity_id = \"store_true\"\n",
    "    args.entity_id_size = 20\n",
    "\n",
    "    args.nlayers = 1\n",
    "    args.lstm_hidden_size = 32\n",
    "    args.lstm_dropout = 0.4\n",
    "\n",
    "    args.lr = 0.001\n",
    "    args.batch_size = 1\n",
    "    args.test_batch_size = 1\n",
    "    args.epoch = 40\n",
    "    args.test_epoch = 5\n",
    "    args.weight_decay = 0.0001\n",
    "    args.negativa_alpha = 4\n",
    "    args.save_model_freq = 1\n",
    "\n",
    "    args.gcn_layers = 2\n",
    "    args.gcn_dim = 128\n",
    "    args.dropout = 0.6\n",
    "    args.activation = \"relu\"\n",
    "\n",
    "    args.bert_hid_size = 768\n",
    "    args.coslr = \"store_true\"\n",
    "\n",
    "    args.use_model = \"bert\"\n",
    "\n",
    "    args.input_theta = 1.0\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "args = config()\n",
    "\n",
    "data_opt = Object()\n",
    "data_opt.data_dir = \"/home/hwiric/SagDRE-2024-07-05/data/DocRED\"\n",
    "data_opt.rel2id = json.load(open(os.path.join(data_opt.data_dir, \"rel2id.json\"), \"r\"))\n",
    "data_opt.id2rel = {v: k for k, v in data_opt.rel2id.items()}\n",
    "data_opt.word2id = json.load(open(os.path.join(data_opt.data_dir, \"word2id.json\"), \"r\"))\n",
    "data_opt.ner2id = json.load(open(os.path.join(data_opt.data_dir, \"ner2id.json\"), \"r\"))\n",
    "data_opt.word2vec = np.load(os.path.join(data_opt.data_dir, \"vec.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwiric/anaconda3/envs/2024-05-10-gnn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hwiric/anaconda3/envs/2024-05-10-gnn/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352645774/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from tqdm.std import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import nn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def get_cuda(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        return tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class Bert:\n",
    "    MASK = \"[MASK]\"\n",
    "    CLS = \"[CLS]\"\n",
    "    SEP = \"[SEP]\"\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        if model_name == \"SAGDRE_BERT_base\":\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "        self.max_len = 512\n",
    "\n",
    "    def tokenize(self, text, masked_idxs=None):\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        if masked_idxs is not None:\n",
    "            for idx in masked_idxs:\n",
    "                tokenized_text[idx] = self.MASK\n",
    "        tokenized = [self.CLS] + tokenized_text + [self.SEP]\n",
    "        return tokenized\n",
    "\n",
    "    def tokenize_to_ids(self, text, masked_idxs=None, pad=True):\n",
    "        tokens = self.tokenize(text, masked_idxs)\n",
    "        return tokens, self.convert_tokens_to_ids(tokens, pad=pad)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens, pad=True):\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        ids = torch.tensor([token_ids])\n",
    "        ids = ids[:, : self.max_len]\n",
    "        if pad:\n",
    "            padded_ids = torch.zeros(1, self.max_len).to(ids)\n",
    "            padded_ids[0, : ids.size(1)] = ids\n",
    "            mask = torch.zeros(1, self.max_len).to(ids)\n",
    "            mask[0, : ids.size(1)] = 1\n",
    "            return padded_ids, mask\n",
    "        else:\n",
    "            return ids\n",
    "\n",
    "    def flatten(self, list_of_lists):\n",
    "        for list in list_of_lists:\n",
    "            for item in list:\n",
    "                yield item\n",
    "\n",
    "    def subword_tokenize(self, tokens):\n",
    "        \"\"\"Segment each token into subwords while keeping track of\n",
    "        token boundaries.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens: A sequence of strings, representing input tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        A tuple consisting of:\n",
    "            - A list of subwords, flanked by the special symbols required\n",
    "                by Bert (CLS and SEP).\n",
    "            - An array of indices into the list of subwords, indicating\n",
    "                that the corresponding subword is the start of a new\n",
    "                token. For example, [1, 3, 4, 7] means that the subwords\n",
    "                1, 3, 4, 7 are token starts, while all other subwords\n",
    "                (0, 2, 5, 6, 8...) are in or at the end of tokens.\n",
    "                This list allows selecting Bert hidden states that\n",
    "                represent tokens, which is necessary in sequence\n",
    "                labeling.\n",
    "        \"\"\"\n",
    "        subwords = list(map(self.tokenizer.tokenize, tokens))\n",
    "        subword_lengths = list(map(len, subwords))\n",
    "        subwords = [self.CLS] + list(self.flatten(subwords))[:509] + [self.SEP]\n",
    "        token_start_idxs = 1 + np.cumsum([0] + subword_lengths[:-1])\n",
    "        token_start_idxs[token_start_idxs > 509] = 512\n",
    "        return subwords, token_start_idxs\n",
    "\n",
    "    def subword_tokenize_to_ids(self, tokens):\n",
    "        \"\"\"Segment each token into subwords while keeping track of\n",
    "        token boundaries and convert subwords into IDs.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens: A sequence of strings, representing input tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        A tuple consisting of:\n",
    "            - A list of subword IDs, including IDs of the special\n",
    "                symbols (CLS and SEP) required by Bert.\n",
    "            - A mask indicating padding tokens.\n",
    "            - An array of indices into the list of subwords. See\n",
    "                doc of subword_tokenize.\n",
    "        \"\"\"\n",
    "        subwords, token_start_idxs = self.subword_tokenize(tokens)\n",
    "        subword_ids, mask = self.convert_tokens_to_ids(subwords)\n",
    "        return subword_ids.numpy(), token_start_idxs, subwords\n",
    "\n",
    "    def segment_ids(self, segment1_len, segment2_len):\n",
    "        ids = [0] * segment1_len + [1] * segment2_len\n",
    "        return torch.tensor([ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import dgl\n",
    "from spacy.tokens import Doc\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp.tokenizer = nlp.tokenizer.tokens_from_list\n",
    "\n",
    "\n",
    "def build_g(sentences, pos_idx, max_id):\n",
    "    # sentences should be a list of word lists\n",
    "    # [[sent_1], [sent_2], ..., [sent_m]]\n",
    "    # senti = [w_0, w_1, ..., w_n]\n",
    "    pre_roots = []\n",
    "    g = nx.DiGraph()\n",
    "    docs = [Doc(nlp.vocab, words=ws) for ws in sentences]\n",
    "    # tokens = parser(doc)\n",
    "    for tokens in nlp.pipe(docs):\n",
    "        g, pre_roots = parse_sent(tokens, g, pre_roots)\n",
    "    # g = add_same_words_links(g, remove_stopwords=True)\n",
    "    g, start = add_entity_node(g, pos_idx, max_id)\n",
    "    paths = get_entity_paths(g, max_id, start)\n",
    "    g = dgl.from_networkx(g)\n",
    "    return g, paths\n",
    "\n",
    "\n",
    "def parse_sent(tokens, g, pre_roots):\n",
    "    roots = [token for token in tokens if token.head == token]\n",
    "    start = len(g.nodes())\n",
    "    dic = {}\n",
    "    idx = 0\n",
    "    for token in tokens:\n",
    "        is_root = token in roots\n",
    "        g.add_node(\n",
    "            start + idx,\n",
    "            text=token.text,\n",
    "            vector=token.vector,\n",
    "            is_root=is_root,\n",
    "            tag=token.tag_,\n",
    "            pos=token.pos_,\n",
    "            dep=token.dep_,\n",
    "        )\n",
    "        dic[token] = start + idx\n",
    "        idx += 1\n",
    "\n",
    "    for token in tokens:\n",
    "        g.add_edge(dic[token], dic[token.head], dep=token.dep_)\n",
    "        g.add_edge(dic[token.head], dic[token], dep=token.dep_)\n",
    "\n",
    "    for idx, root in enumerate(roots[:-1]):\n",
    "        g.add_edge(dic[root], dic[roots[idx + 1]], dep=token.dep_)\n",
    "        g.add_edge(dic[roots[idx + 1]], dic[root], dep=token.dep_)\n",
    "\n",
    "    if pre_roots:\n",
    "        pre_root_idx = pre_roots[-1]\n",
    "        for root in roots[:1]:\n",
    "            g.add_edge(dic[root], pre_root_idx, dep=\"rootconn\")\n",
    "            g.add_edge(pre_root_idx, dic[root], dep=\"rootconn\")\n",
    "    for pre_root_idx in pre_roots:\n",
    "        # pre_root_idx = pre_roots[-2]\n",
    "        # g.add_edge(dic[root], pre_root_idx, dep='rootconn')\n",
    "        for root in roots[:1]:\n",
    "            g.add_edge(pre_root_idx, dic[root], dep=\"rootconn\")\n",
    "    for root in roots[:1]:\n",
    "        pre_roots.append(dic[root])\n",
    "    return g, pre_roots\n",
    "\n",
    "\n",
    "def add_same_words_links(g, remove_stopwords=True):\n",
    "    names = nx.get_node_attributes(g, \"text\")\n",
    "    name_dic = {}\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    for idx, name in names.items():\n",
    "        name = name.lower()\n",
    "        if remove_stopwords and name in stopwords:\n",
    "            continue\n",
    "        if len(name) < 5:\n",
    "            continue\n",
    "        if name not in name_dic:\n",
    "            name_dic[name] = [idx]\n",
    "        else:\n",
    "            for pre_idx in name_dic[name]:\n",
    "                g.add_edge(idx, pre_idx)\n",
    "                g.add_edge(pre_idx, idx)\n",
    "            name_dic[name].append(idx)\n",
    "    return g\n",
    "\n",
    "\n",
    "def add_entity_node(g, pos_idx, max_id):\n",
    "    start = len(g.nodes())\n",
    "    for idx in range(max_id):\n",
    "        g.add_node(start + idx, text=\"entity_%s\" % idx, is_root=False)\n",
    "        if idx + 1 not in pos_idx:\n",
    "            continue\n",
    "        for idx2, node in enumerate(pos_idx[idx + 1]):\n",
    "            g.add_edge(start + idx, node)\n",
    "            g.add_edge(node, start + idx)\n",
    "    return g, start\n",
    "\n",
    "\n",
    "def get_entity_paths(g, max_id, start):\n",
    "    ent_paths = {}\n",
    "    pos_data = nx.get_node_attributes(g, \"pos\")\n",
    "    for idx in range(max_id):\n",
    "        for j in range(max_id):\n",
    "            if idx != j:\n",
    "                try:\n",
    "                    # paths = list(nx.all_simple_paths(g, start + idx, start + j, 8))\n",
    "                    # if len(paths) == 0:\n",
    "                    paths = [nx.shortest_path(g, start + idx, start + j)]\n",
    "                except nx.NetworkXNoPath:\n",
    "                    paths = []\n",
    "                new_paths = []\n",
    "                # for path in paths:\n",
    "                #     path = sorted(path)\n",
    "                #     new_paths.append(path)\n",
    "\n",
    "                for path in paths[:3]:\n",
    "                    # add immediate neighbors for nodes on the path\n",
    "                    neighbors = set()\n",
    "                    # neighbors = list()\n",
    "                    for n in path:\n",
    "                        neighbors.add(n)\n",
    "                        for cur_neigh in g.neighbors(n):\n",
    "                            if cur_neigh in pos_data:\n",
    "                                if pos_data[cur_neigh] in [\"ADP\"]:\n",
    "                                    neighbors.add(cur_neigh)\n",
    "                                # if dep_data[cur_neigh] in ['neg']:\n",
    "                                #     neighbors.add(cur_neigh)\n",
    "                    path = sorted(neighbors)\n",
    "                    # path = sorted(path)\n",
    "                    new_paths.append(path)\n",
    "                # except nx.NetworkXNoPath:\n",
    "                #     new_paths = [[start + idx] + roots_path + [start + j]]\n",
    "                #     # path = [start+idx] + roots_path + [start+j]\n",
    "                #     # path = [start+idx, start+j]\n",
    "\n",
    "                ent_paths[(idx + 1, j + 1)] = new_paths[:3]\n",
    "    return ent_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "\n",
    "class BERTDGLREDataset(IterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_file,\n",
    "        ner2id,\n",
    "        rel2id,\n",
    "        dataset=\"train\",\n",
    "        instance_in_train=None,\n",
    "        model_name=\"SAGDRE_BERT_base\",\n",
    "    ):\n",
    "\n",
    "        super(BERTDGLREDataset, self).__init__()\n",
    "\n",
    "        if instance_in_train is None:\n",
    "            self.instance_in_train = set()\n",
    "        else:\n",
    "            self.instance_in_train = instance_in_train\n",
    "        self.data = None\n",
    "        self.document_max_length = 512\n",
    "        self.bert = Bert(model_name)\n",
    "        self.dataset = dataset\n",
    "        self.rel2id = rel2id\n",
    "        self.ner2id = ner2id\n",
    "        print(\"Reading data from {}.\".format(src_file))\n",
    "\n",
    "        self.create_data(src_file)\n",
    "        self.get_instance_in_train()\n",
    "\n",
    "    def get_instance_in_train(self):\n",
    "        for doc in self.data:\n",
    "            entity_list = doc[\"vertexSet\"]\n",
    "            labels = doc.get(\"labels\", [])\n",
    "            for label in labels:\n",
    "                head, tail, relation = label[\"h\"], label[\"t\"], label[\"r\"]\n",
    "                label[\"r\"] = self.rel2id[relation]\n",
    "                if self.dataset == \"train\":\n",
    "                    for n1 in entity_list[head]:\n",
    "                        for n2 in entity_list[tail]:\n",
    "                            mention_triple = (n1[\"name\"], n2[\"name\"], relation)\n",
    "                            self.instance_in_train.add(mention_triple)\n",
    "\n",
    "    def process_doc(self, doc, dataset, ner2id, bert):\n",
    "        title, entity_list = doc[\"title\"], doc[\"vertexSet\"]\n",
    "        labels, sentences = doc.get(\"labels\", []), doc[\"sents\"]\n",
    "\n",
    "        Ls = [0]\n",
    "        L = 0\n",
    "        for x in sentences:\n",
    "            L += len(x)\n",
    "            Ls.append(L)\n",
    "        for j in range(len(entity_list)):\n",
    "            for k in range(len(entity_list[j])):\n",
    "                sent_id = int(entity_list[j][k][\"sent_id\"])\n",
    "                entity_list[j][k][\"sent_id\"] = sent_id\n",
    "\n",
    "                dl = Ls[sent_id]\n",
    "                pos0, pos1 = entity_list[j][k][\"pos\"]\n",
    "                entity_list[j][k][\"global_pos\"] = (pos0 + dl, pos1 + dl)\n",
    "\n",
    "        # generate positive examples\n",
    "        train_triple = []\n",
    "        new_labels = []\n",
    "        for label in labels:\n",
    "            head, tail, relation = label[\"h\"], label[\"t\"], label[\"r\"]\n",
    "            # label['r'] = rel2id[relation]\n",
    "            train_triple.append((head, tail))\n",
    "            label[\"in_train\"] = False\n",
    "\n",
    "            # record training set mention triples and mark for dev and test\n",
    "            for n1 in entity_list[head]:\n",
    "                for n2 in entity_list[tail]:\n",
    "                    mention_triple = (n1[\"name\"], n2[\"name\"], relation)\n",
    "                    if dataset != \"train\":\n",
    "                        if mention_triple in self.instance_in_train:\n",
    "                            label[\"in_train\"] = True\n",
    "                            break\n",
    "\n",
    "            new_labels.append(label)\n",
    "\n",
    "        # generate negative examples\n",
    "        na_triple = []\n",
    "        for j in range(len(entity_list)):\n",
    "            for k in range(len(entity_list)):\n",
    "                if j != k and (j, k) not in train_triple:\n",
    "                    na_triple.append((j, k))\n",
    "\n",
    "        # generate document ids\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                words.append(word)\n",
    "\n",
    "        bert_token, bert_starts, bert_subwords = bert.subword_tokenize_to_ids(words)\n",
    "\n",
    "        word_id = np.zeros((self.document_max_length,), dtype=np.int32)\n",
    "        pos_id = np.zeros((self.document_max_length,), dtype=np.int32)\n",
    "        ner_id = np.zeros((self.document_max_length,), dtype=np.int32)\n",
    "        mention_id = np.zeros((self.document_max_length,), dtype=np.int32)\n",
    "        word_id[:] = bert_token[0]\n",
    "\n",
    "        entity2mention = defaultdict(list)\n",
    "        mention_idx = 1\n",
    "        already_exist = set()\n",
    "        pos_idx = {}\n",
    "        ent_idx = {}\n",
    "        for idx, vertex in enumerate(entity_list, 1):\n",
    "            for v in vertex:\n",
    "\n",
    "                sent_id, ner_type = v[\"sent_id\"], v[\"type\"]\n",
    "                pos0_w, pos1_w = v[\"global_pos\"]\n",
    "\n",
    "                pos0 = bert_starts[pos0_w]\n",
    "                if pos1_w < len(bert_starts):\n",
    "                    pos1 = bert_starts[pos1_w]\n",
    "                else:\n",
    "                    pos1 = self.document_max_length\n",
    "\n",
    "                if (pos0, pos1) in already_exist:\n",
    "                    continue\n",
    "\n",
    "                if pos0 >= len(pos_id):\n",
    "                    continue\n",
    "\n",
    "                if idx not in pos_idx:\n",
    "                    pos_idx[idx] = []\n",
    "                    ent_idx[idx] = []\n",
    "\n",
    "                pos_idx[idx].extend(range(pos0_w, pos1_w))\n",
    "                ent_idx[idx].extend(range(pos0, pos1))\n",
    "                pos_id[pos0:pos1] = idx\n",
    "                ner_id[pos0:pos1] = ner2id[ner_type]\n",
    "                mention_id[pos0:pos1] = mention_idx\n",
    "                entity2mention[idx].append(mention_idx)\n",
    "                mention_idx += 1\n",
    "                already_exist.add((pos0, pos1))\n",
    "\n",
    "        # ======================================================\n",
    "        # compute subword to word index\n",
    "        sub2word = np.zeros(\n",
    "            (len(bert_starts) + len(entity_list), self.document_max_length)\n",
    "        )\n",
    "        for idx in range(len(bert_starts) - 1):\n",
    "            start, end = bert_starts[idx], bert_starts[idx + 1]\n",
    "            if start == end:\n",
    "                continue\n",
    "            sub2word[idx, start:end] = 1 / (end - start)\n",
    "        start, end = bert_starts[-1], len(bert_subwords)\n",
    "        sub2word[len(bert_starts) - 1, start:end] = 1 / (end - start)\n",
    "        # compute convertion matrix for entity\n",
    "        for idx, poss in ent_idx.items():\n",
    "            # print('------------>', idx, poss)\n",
    "            sub2word[len(bert_starts) + idx - 1, poss] = 1 / len(poss)\n",
    "        # ======================================================\n",
    "        # compute words to sent index\n",
    "        word2sent = np.zeros((len(Ls) - 1, Ls[-1]))\n",
    "        for i in range(1, len(Ls)):\n",
    "            word2sent[i - 1, Ls[i - 1] : Ls[i]] = 1 / (Ls[i] - Ls[i - 1])\n",
    "        # ======================================================\n",
    "\n",
    "        replace_i = 0\n",
    "        idx = len(entity_list)\n",
    "        if entity2mention[idx] == []:\n",
    "            entity2mention[idx].append(mention_idx)\n",
    "            while mention_id[replace_i] != 0:\n",
    "                replace_i += 1\n",
    "            mention_id[replace_i] = mention_idx\n",
    "            pos_id[replace_i] = idx\n",
    "            ner_id[replace_i] = ner2id[vertex[0][\"type\"]]\n",
    "            mention_idx += 1\n",
    "\n",
    "        new_Ls = [0]\n",
    "        for ii in range(1, len(Ls)):\n",
    "            if Ls[ii] < len(bert_starts):\n",
    "                new_Ls.append(bert_starts[Ls[ii]])\n",
    "            else:\n",
    "                new_Ls.append(len(bert_subwords))\n",
    "\n",
    "        Ls = new_Ls\n",
    "\n",
    "        graph2, path2 = build_g(sentences, pos_idx, pos_id.max())\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"num_sent\": len(doc[\"sents\"]),\n",
    "            \"entities\": entity_list,\n",
    "            \"labels\": new_labels,\n",
    "            \"na_triple\": na_triple,\n",
    "            \"word_id\": word_id,\n",
    "            \"pos_id\": pos_id,\n",
    "            \"ner_id\": ner_id,\n",
    "            \"sub2word\": sub2word,\n",
    "            \"word2sent\": word2sent,\n",
    "            \"graph2\": graph2,\n",
    "            \"path2\": path2,\n",
    "        }\n",
    "\n",
    "    def create_data(self, src_file):\n",
    "        with open(file=src_file, mode=\"r\", encoding=\"utf-8\") as fr:\n",
    "            ori_data = json.load(fr)\n",
    "        self.data = ori_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.data[idx]\n",
    "        cur_d = self.process_doc(\n",
    "            doc, dataset=self.dataset, ner2id=self.ner2id, bert=self.bert\n",
    "        )\n",
    "        return cur_d\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwiric/anaconda3/envs/2024-05-10-gnn/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from /home/hwiric/SagDRE-2024-07-05/data/DocRED/train_annotated.json.\n",
      "Reading data from /home/hwiric/SagDRE-2024-07-05/data/DocRED/dev.json.\n"
     ]
    }
   ],
   "source": [
    "train_set = BERTDGLREDataset(\n",
    "    args.train_set,\n",
    "    data_opt.ner2id,\n",
    "    data_opt.rel2id,\n",
    "    dataset=\"train\",\n",
    "    model_name=args.model_name,\n",
    ")\n",
    "\n",
    "dev_set = BERTDGLREDataset(\n",
    "    args.dev_set,\n",
    "    data_opt.ner2id,\n",
    "    data_opt.rel2id,\n",
    "    dataset=\"dev\",\n",
    "    instance_in_train=train_set.instance_in_train,\n",
    "    model_name=args.model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "class SampleEncoder:\n",
    "    def __init__(self, words_id) -> None:\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\", return_dict=False)\n",
    "        self.words_id = torch.unsqueeze(torch.from_numpy(words_id), 0)\n",
    "        self.mask = torch.unsqueeze(torch.from_numpy(words_id > 0), 0)\n",
    "\n",
    "    def encode(self):\n",
    "        output, x = self.bert(input_ids=self.words_id, attention_mask=self.mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def convert_edges_format_dgl2dict(src, dst):\n",
    "    src = src.detach().cpu().numpy()\n",
    "    dst = dst.detach().cpu().numpy()\n",
    "\n",
    "    edges = defaultdict(list)\n",
    "    for element in list(zip(src, dst)):\n",
    "        edges[element[0]].append(element[1])\n",
    "\n",
    "    return edges\n",
    "\n",
    "\n",
    "def raw_data(src_file):\n",
    "    with open(file=src_file, mode=\"r\", encoding=\"utf-8\") as fr:\n",
    "        ori_data = json.load(fr)\n",
    "\n",
    "    return ori_data\n",
    "\n",
    "\n",
    "def subwords_cnt_dist(raw_data):\n",
    "    my_bert = Bert(\"SAGDRE_BERT_base\")\n",
    "    subwords_cnt = []\n",
    "\n",
    "    for sample in raw_data:\n",
    "        words = []\n",
    "        for sentence in sample[\"sents\"]:\n",
    "            for word in sentence:\n",
    "                words.append(word)\n",
    "\n",
    "        bert_token, bert_starts, bert_subwords = my_bert.subword_tokenize_to_ids(words)\n",
    "        subwords_cnt.append(len(bert_subwords))\n",
    "\n",
    "    return subwords_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024-05-10-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
