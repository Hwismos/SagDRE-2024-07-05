{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def config():\n",
    "    args = EasyDict()\n",
    "    args.train_set = \"/home/hwiric/SagDRE-2024-07-05/data/DocRED/train_annotated.json\"\n",
    "    args.dev_set = \"/home/hwiric/SagDRE-2024-07-05/data/DocRED/dev.json\"\n",
    "    args.test_set = \"/home/hwiric/SagDRE-2024-07-05/data/DocRED/test.json\"\n",
    "\n",
    "    args.checkpoint_dir = \"checkpoint\"\n",
    "    args.model_name = \"SAGDRE_BERT_base\"\n",
    "    args.pretrain_model = \"\"\n",
    "\n",
    "    args.relation_nums = 97\n",
    "    args.entity_type_num = 7\n",
    "    args.max_entity_num = 80\n",
    "\n",
    "    args.word_pad = 0\n",
    "    args.entity_type_pad = 0\n",
    "    args.entity_id_pad = 0\n",
    "\n",
    "    args.word_emb_size = 10\n",
    "    args.use_entity_type = \"store_true\"\n",
    "    args.entity_type_size = 20\n",
    "\n",
    "    args.use_entity_id = \"store_true\"\n",
    "    args.entity_id_size = 20\n",
    "\n",
    "    args.nlayers = 1\n",
    "    args.lstm_hidden_size = 32\n",
    "    args.lstm_dropout = 0.4\n",
    "\n",
    "    args.lr = 0.001\n",
    "    args.batch_size = 2\n",
    "    args.test_batch_size = 1\n",
    "    args.epoch = 40\n",
    "    args.test_epoch = 5\n",
    "    args.weight_decay = 0.0001\n",
    "    args.negativa_alpha = 4\n",
    "    args.save_model_freq = 1\n",
    "\n",
    "    args.gcn_layers = 2\n",
    "    args.gcn_dim = 128\n",
    "    args.dropout = 0.6\n",
    "    args.activation = \"relu\"\n",
    "\n",
    "    args.bert_hid_size = 768\n",
    "    args.coslr = \"store_true\"\n",
    "\n",
    "    args.use_model = \"bert\"\n",
    "\n",
    "    args.input_theta = 1.0\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "args = config()\n",
    "\n",
    "data_opt = Object()\n",
    "data_opt.data_dir = \"/home/hwiric/SagDRE-2024-07-05/data/DocRED\"\n",
    "data_opt.rel2id = json.load(open(os.path.join(data_opt.data_dir, \"rel2id.json\"), \"r\"))\n",
    "data_opt.id2rel = {v: k for k, v in data_opt.rel2id.items()}\n",
    "data_opt.word2id = json.load(open(os.path.join(data_opt.data_dir, \"word2id.json\"), \"r\"))\n",
    "data_opt.ner2id = json.load(open(os.path.join(data_opt.data_dir, \"ner2id.json\"), \"r\"))\n",
    "data_opt.word2vec = np.load(os.path.join(data_opt.data_dir, \"vec.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from tqdm.std import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import nn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def get_cuda(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        return tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class Bert:\n",
    "    MASK = \"[MASK]\"\n",
    "    CLS = \"[CLS]\"\n",
    "    SEP = \"[SEP]\"\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        if model_name == \"SAGDRE_BERT_base\":\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(\"bert-large-cased\")\n",
    "        self.max_len = 512\n",
    "\n",
    "    def tokenize(self, text, masked_idxs=None):\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        if masked_idxs is not None:\n",
    "            for idx in masked_idxs:\n",
    "                tokenized_text[idx] = self.MASK\n",
    "        tokenized = [self.CLS] + tokenized_text + [self.SEP]\n",
    "        return tokenized\n",
    "\n",
    "    def tokenize_to_ids(self, text, masked_idxs=None, pad=True):\n",
    "        tokens = self.tokenize(text, masked_idxs)\n",
    "        return tokens, self.convert_tokens_to_ids(tokens, pad=pad)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens, pad=True):\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        ids = torch.tensor([token_ids])\n",
    "        ids = ids[:, : self.max_len]\n",
    "        if pad:\n",
    "            padded_ids = torch.zeros(1, self.max_len).to(ids)\n",
    "            padded_ids[0, : ids.size(1)] = ids\n",
    "            mask = torch.zeros(1, self.max_len).to(ids)\n",
    "            mask[0, : ids.size(1)] = 1\n",
    "            return padded_ids, mask\n",
    "        else:\n",
    "            return ids\n",
    "\n",
    "    def flatten(self, list_of_lists):\n",
    "        for list in list_of_lists:\n",
    "            for item in list:\n",
    "                yield item\n",
    "\n",
    "    def subword_tokenize(self, tokens):\n",
    "        \"\"\"Segment each token into subwords while keeping track of\n",
    "        token boundaries.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens: A sequence of strings, representing input tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        A tuple consisting of:\n",
    "            - A list of subwords, flanked by the special symbols required\n",
    "                by Bert (CLS and SEP).\n",
    "            - An array of indices into the list of subwords, indicating\n",
    "                that the corresponding subword is the start of a new\n",
    "                token. For example, [1, 3, 4, 7] means that the subwords\n",
    "                1, 3, 4, 7 are token starts, while all other subwords\n",
    "                (0, 2, 5, 6, 8...) are in or at the end of tokens.\n",
    "                This list allows selecting Bert hidden states that\n",
    "                represent tokens, which is necessary in sequence\n",
    "                labeling.\n",
    "        \"\"\"\n",
    "        subwords = list(map(self.tokenizer.tokenize, tokens))\n",
    "        subword_lengths = list(map(len, subwords))\n",
    "        subwords = [self.CLS] + list(self.flatten(subwords))[:509] + [self.SEP]\n",
    "        token_start_idxs = 1 + np.cumsum([0] + subword_lengths[:-1])\n",
    "        token_start_idxs[token_start_idxs > 509] = 512\n",
    "        return subwords, token_start_idxs\n",
    "\n",
    "    def subword_tokenize_to_ids(self, tokens):\n",
    "        \"\"\"Segment each token into subwords while keeping track of\n",
    "        token boundaries and convert subwords into IDs.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens: A sequence of strings, representing input tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        A tuple consisting of:\n",
    "            - A list of subword IDs, including IDs of the special\n",
    "                symbols (CLS and SEP) required by Bert.\n",
    "            - A mask indicating padding tokens.\n",
    "            - An array of indices into the list of subwords. See\n",
    "                doc of subword_tokenize.\n",
    "        \"\"\"\n",
    "        subwords, token_start_idxs = self.subword_tokenize(tokens)\n",
    "        subword_ids, mask = self.convert_tokens_to_ids(subwords)\n",
    "        return subword_ids.numpy(), token_start_idxs, subwords\n",
    "\n",
    "    def segment_ids(self, segment1_len, segment2_len):\n",
    "        ids = [0] * segment1_len + [1] * segment2_len\n",
    "        return torch.tensor([ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import dgl\n",
    "from spacy.tokens import Doc\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp.tokenizer = nlp.tokenizer.tokens_from_list\n",
    "\n",
    "\n",
    "def build_g(sentences, pos_idx, max_id):\n",
    "    # sentences should be a list of word lists\n",
    "    # [[sent_1], [sent_2], ..., [sent_m]]\n",
    "    # senti = [w_0, w_1, ..., w_n]\n",
    "    pre_roots = []\n",
    "    g = nx.DiGraph()\n",
    "    docs = [Doc(nlp.vocab, words=ws) for ws in sentences]\n",
    "    # tokens = parser(doc)\n",
    "    for tokens in nlp.pipe(docs):\n",
    "        g, pre_roots = parse_sent(tokens, g, pre_roots)\n",
    "    # g = add_same_words_links(g, remove_stopwords=True)\n",
    "    g, start = add_entity_node(g, pos_idx, max_id)\n",
    "    paths = get_entity_paths(g, max_id, start)\n",
    "    g = dgl.from_networkx(g)\n",
    "    return g, paths\n",
    "\n",
    "\n",
    "def parse_sent(tokens, g, pre_roots):\n",
    "    roots = [token for token in tokens if token.head == token]\n",
    "    start = len(g.nodes())\n",
    "    dic = {}\n",
    "    idx = 0\n",
    "    for token in tokens:\n",
    "        is_root = token in roots\n",
    "        g.add_node(\n",
    "            start + idx,\n",
    "            text=token.text,\n",
    "            vector=token.vector,\n",
    "            is_root=is_root,\n",
    "            tag=token.tag_,\n",
    "            pos=token.pos_,\n",
    "            dep=token.dep_,\n",
    "        )\n",
    "        dic[token] = start + idx\n",
    "        idx += 1\n",
    "\n",
    "    for token in tokens:\n",
    "        g.add_edge(dic[token], dic[token.head], dep=token.dep_)\n",
    "        g.add_edge(dic[token.head], dic[token], dep=token.dep_)\n",
    "\n",
    "    for idx, root in enumerate(roots[:-1]):\n",
    "        g.add_edge(dic[root], dic[roots[idx + 1]], dep=token.dep_)\n",
    "        g.add_edge(dic[roots[idx + 1]], dic[root], dep=token.dep_)\n",
    "\n",
    "    if pre_roots:\n",
    "        pre_root_idx = pre_roots[-1]\n",
    "        for root in roots[:1]:\n",
    "            g.add_edge(dic[root], pre_root_idx, dep=\"rootconn\")\n",
    "            g.add_edge(pre_root_idx, dic[root], dep=\"rootconn\")\n",
    "    for pre_root_idx in pre_roots:\n",
    "        # pre_root_idx = pre_roots[-2]\n",
    "        # g.add_edge(dic[root], pre_root_idx, dep='rootconn')\n",
    "        for root in roots[:1]:\n",
    "            g.add_edge(pre_root_idx, dic[root], dep=\"rootconn\")\n",
    "    for root in roots[:1]:\n",
    "        pre_roots.append(dic[root])\n",
    "    return g, pre_roots\n",
    "\n",
    "\n",
    "def add_same_words_links(g, remove_stopwords=True):\n",
    "    names = nx.get_node_attributes(g, \"text\")\n",
    "    name_dic = {}\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    for idx, name in names.items():\n",
    "        name = name.lower()\n",
    "        if remove_stopwords and name in stopwords:\n",
    "            continue\n",
    "        if len(name) < 5:\n",
    "            continue\n",
    "        if name not in name_dic:\n",
    "            name_dic[name] = [idx]\n",
    "        else:\n",
    "            for pre_idx in name_dic[name]:\n",
    "                g.add_edge(idx, pre_idx)\n",
    "                g.add_edge(pre_idx, idx)\n",
    "            name_dic[name].append(idx)\n",
    "    return g\n",
    "\n",
    "\n",
    "def add_entity_node(g, pos_idx, max_id):\n",
    "    start = len(g.nodes())\n",
    "    for idx in range(max_id):\n",
    "        g.add_node(start + idx, text=\"entity_%s\" % idx, is_root=False)\n",
    "        if idx + 1 not in pos_idx:\n",
    "            continue\n",
    "        for idx2, node in enumerate(pos_idx[idx + 1]):\n",
    "            g.add_edge(start + idx, node)\n",
    "            g.add_edge(node, start + idx)\n",
    "    return g, start\n",
    "\n",
    "\n",
    "def get_entity_paths(g, max_id, start):\n",
    "    ent_paths = {}\n",
    "    pos_data = nx.get_node_attributes(g, \"pos\")\n",
    "    for idx in range(max_id):\n",
    "        for j in range(max_id):\n",
    "            if idx != j:\n",
    "                try:\n",
    "                    # paths = list(nx.all_simple_paths(g, start + idx, start + j, 8))\n",
    "                    # if len(paths) == 0:\n",
    "                    paths = [nx.shortest_path(g, start + idx, start + j)]\n",
    "                except nx.NetworkXNoPath:\n",
    "                    paths = []\n",
    "                new_paths = []\n",
    "                # for path in paths:\n",
    "                #     path = sorted(path)\n",
    "                #     new_paths.append(path)\n",
    "\n",
    "                for path in paths[:3]:\n",
    "                    # add immediate neighbors for nodes on the path\n",
    "                    neighbors = set()\n",
    "                    # neighbors = list()\n",
    "                    for n in path:\n",
    "                        neighbors.add(n)\n",
    "                        for cur_neigh in g.neighbors(n):\n",
    "                            if cur_neigh in pos_data:\n",
    "                                if pos_data[cur_neigh] in [\"ADP\"]:\n",
    "                                    neighbors.add(cur_neigh)\n",
    "                                # if dep_data[cur_neigh] in ['neg']:\n",
    "                                #     neighbors.add(cur_neigh)\n",
    "                    path = sorted(neighbors)\n",
    "                    # path = sorted(path)\n",
    "                    new_paths.append(path)\n",
    "                # except nx.NetworkXNoPath:\n",
    "                #     new_paths = [[start + idx] + roots_path + [start + j]]\n",
    "                #     # path = [start+idx] + roots_path + [start+j]\n",
    "                #     # path = [start+idx, start+j]\n",
    "\n",
    "                ent_paths[(idx + 1, j + 1)] = new_paths[:3]\n",
    "    return ent_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "\n",
    "class BERTDGLREDataset(IterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_file,\n",
    "        ner2id,\n",
    "        rel2id,\n",
    "        dataset=\"train\",\n",
    "        instance_in_train=None,\n",
    "        model_name=\"SAGDRE_BERT_base\",\n",
    "    ):\n",
    "\n",
    "        super(BERTDGLREDataset, self).__init__()\n",
    "\n",
    "        if instance_in_train is None:\n",
    "            self.instance_in_train = set()\n",
    "        else:\n",
    "            self.instance_in_train = instance_in_train\n",
    "        self.data = None\n",
    "        self.document_max_length = 512\n",
    "        self.bert = Bert(model_name)\n",
    "        self.dataset = dataset\n",
    "        self.rel2id = rel2id\n",
    "        self.ner2id = ner2id\n",
    "        print(\"Reading data from {}.\".format(src_file))\n",
    "\n",
    "        self.create_data(src_file)\n",
    "        self.get_instance_in_train()\n",
    "\n",
    "    def get_instance_in_train(self):\n",
    "        for doc in self.data:\n",
    "            entity_list = doc[\"vertexSet\"]\n",
    "            labels = doc.get(\"labels\", [])\n",
    "            for label in labels:\n",
    "                head, tail, relation = label[\"h\"], label[\"t\"], label[\"r\"]\n",
    "                label[\"r\"] = self.rel2id[relation]\n",
    "                if self.dataset == \"train\":\n",
    "                    for n1 in entity_list[head]:\n",
    "                        for n2 in entity_list[tail]:\n",
    "                            mention_triple = (n1[\"name\"], n2[\"name\"], relation)\n",
    "                            self.instance_in_train.add(mention_triple)\n",
    "\n",
    "    def process_doc(self, doc, dataset, ner2id, bert):\n",
    "        title, entity_list = doc[\"title\"], doc[\"vertexSet\"]\n",
    "        labels, sentences = doc.get(\"labels\", []), doc[\"sents\"]\n",
    "\n",
    "        Ls = [0]\n",
    "        L = 0\n",
    "        for x in sentences:\n",
    "            L += len(x)\n",
    "            Ls.append(L)\n",
    "        for j in range(len(entity_list)):\n",
    "            for k in range(len(entity_list[j])):\n",
    "                sent_id = int(entity_list[j][k][\"sent_id\"])\n",
    "                entity_list[j][k][\"sent_id\"] = sent_id\n",
    "\n",
    "                dl = Ls[sent_id]\n",
    "                pos0, pos1 = entity_list[j][k][\"pos\"]\n",
    "                entity_list[j][k][\"global_pos\"] = (pos0 + dl, pos1 + dl)\n",
    "\n",
    "        # generate positive examples\n",
    "        train_triple = []\n",
    "        new_labels = []\n",
    "        for label in labels:\n",
    "            head, tail, relation = label[\"h\"], label[\"t\"], label[\"r\"]\n",
    "            # label['r'] = rel2id[relation]\n",
    "            train_triple.append((head, tail))\n",
    "            label[\"in_train\"] = False\n",
    "\n",
    "            # record training set mention triples and mark for dev and test\n",
    "            for n1 in entity_list[head]:\n",
    "                for n2 in entity_list[tail]:\n",
    "                    mention_triple = (n1[\"name\"], n2[\"name\"], relation)\n",
    "                    if dataset != \"train\":\n",
    "                        if mention_triple in self.instance_in_train:\n",
    "                            label[\"in_train\"] = True\n",
    "                            break\n",
    "\n",
    "            new_labels.append(label)\n",
    "\n",
    "        # generate negative examples\n",
    "        na_triple = []\n",
    "        for j in range(len(entity_list)):\n",
    "            for k in range(len(entity_list)):\n",
    "                if j != k and (j, k) not in train_triple:\n",
    "                    na_triple.append((j, k))\n",
    "\n",
    "        # generate document ids\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                words.append(word)\n",
    "\n",
    "        bert_token, bert_starts, bert_subwords = bert.subword_tokenize_to_ids(words)\n",
    "\n",
    "        word_id = np.zeros((self.document_max_length,), dtype=np.int32)\n",
    "        pos_id = np.zeros((self.document_max_length,), dtype=np.int32)\n",
    "        ner_id = np.zeros((self.document_max_length,), dtype=np.int32)\n",
    "        mention_id = np.zeros((self.document_max_length,), dtype=np.int32)\n",
    "        word_id[:] = bert_token[0]\n",
    "\n",
    "        entity2mention = defaultdict(list)\n",
    "        mention_idx = 1\n",
    "        already_exist = set()\n",
    "        pos_idx = {}\n",
    "        ent_idx = {}\n",
    "        for idx, vertex in enumerate(entity_list, 1):\n",
    "            for v in vertex:\n",
    "\n",
    "                sent_id, ner_type = v[\"sent_id\"], v[\"type\"]\n",
    "                pos0_w, pos1_w = v[\"global_pos\"]\n",
    "\n",
    "                pos0 = bert_starts[pos0_w]\n",
    "                if pos1_w < len(bert_starts):\n",
    "                    pos1 = bert_starts[pos1_w]\n",
    "                else:\n",
    "                    pos1 = self.document_max_length\n",
    "\n",
    "                if (pos0, pos1) in already_exist:\n",
    "                    continue\n",
    "\n",
    "                if pos0 >= len(pos_id):\n",
    "                    continue\n",
    "\n",
    "                if idx not in pos_idx:\n",
    "                    pos_idx[idx] = []\n",
    "                    ent_idx[idx] = []\n",
    "\n",
    "                pos_idx[idx].extend(range(pos0_w, pos1_w))\n",
    "                ent_idx[idx].extend(range(pos0, pos1))\n",
    "                pos_id[pos0:pos1] = idx\n",
    "                ner_id[pos0:pos1] = ner2id[ner_type]\n",
    "                mention_id[pos0:pos1] = mention_idx\n",
    "                entity2mention[idx].append(mention_idx)\n",
    "                mention_idx += 1\n",
    "                already_exist.add((pos0, pos1))\n",
    "\n",
    "        # ======================================================\n",
    "        # compute subword to word index\n",
    "        sub2word = np.zeros(\n",
    "            (len(bert_starts) + len(entity_list), self.document_max_length)\n",
    "        )\n",
    "        for idx in range(len(bert_starts) - 1):\n",
    "            start, end = bert_starts[idx], bert_starts[idx + 1]\n",
    "            if start == end:\n",
    "                continue\n",
    "            sub2word[idx, start:end] = 1 / (end - start)\n",
    "        start, end = bert_starts[-1], len(bert_subwords)\n",
    "        sub2word[len(bert_starts) - 1, start:end] = 1 / (end - start)\n",
    "        # compute convertion matrix for entity\n",
    "        for idx, poss in ent_idx.items():\n",
    "            # print('------------>', idx, poss)\n",
    "            sub2word[len(bert_starts) + idx - 1, poss] = 1 / len(poss)\n",
    "        # ======================================================\n",
    "        # compute words to sent index\n",
    "        word2sent = np.zeros((len(Ls) - 1, Ls[-1]))\n",
    "        for i in range(1, len(Ls)):\n",
    "            word2sent[i - 1, Ls[i - 1] : Ls[i]] = 1 / (Ls[i] - Ls[i - 1])\n",
    "        # ======================================================\n",
    "\n",
    "        replace_i = 0\n",
    "        idx = len(entity_list)\n",
    "        if entity2mention[idx] == []:\n",
    "            entity2mention[idx].append(mention_idx)\n",
    "            while mention_id[replace_i] != 0:\n",
    "                replace_i += 1\n",
    "            mention_id[replace_i] = mention_idx\n",
    "            pos_id[replace_i] = idx\n",
    "            ner_id[replace_i] = ner2id[vertex[0][\"type\"]]\n",
    "            mention_idx += 1\n",
    "\n",
    "        new_Ls = [0]\n",
    "        for ii in range(1, len(Ls)):\n",
    "            if Ls[ii] < len(bert_starts):\n",
    "                new_Ls.append(bert_starts[Ls[ii]])\n",
    "            else:\n",
    "                new_Ls.append(len(bert_subwords))\n",
    "\n",
    "        Ls = new_Ls\n",
    "\n",
    "        graph2, path2 = build_g(sentences, pos_idx, pos_id.max())\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"num_sent\": len(doc[\"sents\"]),\n",
    "            \"entities\": entity_list,\n",
    "            \"labels\": new_labels,\n",
    "            \"na_triple\": na_triple,\n",
    "            \"word_id\": word_id,\n",
    "            \"pos_id\": pos_id,\n",
    "            \"ner_id\": ner_id,\n",
    "            \"sub2word\": sub2word,\n",
    "            \"word2sent\": word2sent,\n",
    "            \"graph2\": graph2,\n",
    "            \"path2\": path2,\n",
    "        }\n",
    "\n",
    "    def create_data(self, src_file):\n",
    "        with open(file=src_file, mode=\"r\", encoding=\"utf-8\") as fr:\n",
    "            ori_data = json.load(fr)\n",
    "        self.data = ori_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.data[idx]\n",
    "        cur_d = self.process_doc(\n",
    "            doc, dataset=self.dataset, ner2id=self.ner2id, bert=self.bert\n",
    "        )\n",
    "        return cur_d\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = BERTDGLREDataset(\n",
    "    args.train_set,\n",
    "    data_opt.ner2id,\n",
    "    data_opt.rel2id,\n",
    "    dataset=\"train\",\n",
    "    model_name=args.model_name,\n",
    ")\n",
    "\n",
    "dev_set = BERTDGLREDataset(\n",
    "    args.dev_set,\n",
    "    data_opt.ner2id,\n",
    "    data_opt.rel2id,\n",
    "    dataset=\"dev\",\n",
    "    instance_in_train=train_set.instance_in_train,\n",
    "    model_name=args.model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class DGLREDataloader(DataLoader):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        shuffle=False,\n",
    "        h_t_limit=1722,\n",
    "        relation_num=97,\n",
    "        max_length=512,\n",
    "        negativa_alpha=0.0,\n",
    "        dataset_type=\"train\",\n",
    "    ):\n",
    "        super(DGLREDataloader, self).__init__(\n",
    "            dataset, batch_size=batch_size, num_workers=4\n",
    "        )\n",
    "        self.shuffle = shuffle\n",
    "        self.length = len(self.dataset)\n",
    "        self.max_length = max_length\n",
    "        self.negativa_alpha = negativa_alpha\n",
    "        self.dataset_type = dataset_type\n",
    "        self.h_t_limit = h_t_limit\n",
    "        self.relation_num = relation_num\n",
    "        self.order = list(range(self.length))\n",
    "        self.data = []\n",
    "        self.boosted = 0\n",
    "        # for idx in tqdm(range(self.length)):\n",
    "        #     self.data.append(self.dataset[idx])\n",
    "        #     self.data[idx][\"idx\"] = idx\n",
    "        with open(\n",
    "            f\"/home/hwiric/SagDRE-2024-07-05/data/DocRED/{self.dataset_type}_data.pkl\",\n",
    "            \"rb\",\n",
    "        ) as handle:\n",
    "            self.data = pickle.load(handle)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.order)\n",
    "        batch_num = math.ceil(self.length / self.batch_size)\n",
    "        self.batches = [\n",
    "            (idx * self.batch_size, min(self.length, (idx + 1) * self.batch_size))\n",
    "            for idx in range(0, batch_num)\n",
    "        ]\n",
    "        self.batches_order = [\n",
    "            self.order[\n",
    "                idx * self.batch_size : min(self.length, (idx + 1) * self.batch_size)\n",
    "            ]\n",
    "            for idx in range(0, batch_num)\n",
    "        ]\n",
    "\n",
    "        # begin\n",
    "        context_word_ids = torch.LongTensor(self.batch_size, self.max_length).cpu()\n",
    "        context_pos_ids = torch.LongTensor(self.batch_size, self.max_length).cpu()\n",
    "        context_ner_ids = torch.LongTensor(self.batch_size, self.max_length).cpu()\n",
    "        context_word_mask = torch.LongTensor(self.batch_size, self.max_length).cpu()\n",
    "        context_word_length = torch.LongTensor(self.batch_size).cpu()\n",
    "        ht_pairs = torch.LongTensor(self.batch_size, self.h_t_limit, 2).cpu()\n",
    "        relation_multi_label = torch.Tensor(\n",
    "            self.batch_size, self.h_t_limit, self.relation_num\n",
    "        ).cpu()\n",
    "        relation_mask = torch.Tensor(self.batch_size, self.h_t_limit).cpu()\n",
    "        relation_example_idx = torch.LongTensor(self.batch_size, self.h_t_limit).cpu()\n",
    "        relation_label = torch.LongTensor(self.batch_size, self.h_t_limit).cpu()\n",
    "\n",
    "        for idx, (batch_s, batch_e) in enumerate(self.batches):\n",
    "            minibatch = [self.data[idx] for idx in self.order[batch_s:batch_e]]\n",
    "            cur_bsz = len(minibatch)\n",
    "\n",
    "            for mapping in [\n",
    "                context_word_ids,\n",
    "                context_pos_ids,\n",
    "                context_ner_ids,\n",
    "                context_word_mask,\n",
    "                context_word_length,\n",
    "                ht_pairs,\n",
    "                relation_multi_label,\n",
    "                relation_mask,\n",
    "                relation_label,\n",
    "                relation_example_idx,\n",
    "            ]:\n",
    "                if mapping is not None:\n",
    "                    mapping.zero_()\n",
    "\n",
    "            relation_label.fill_(IGNORE_INDEX)\n",
    "\n",
    "            max_h_t_cnt = 0\n",
    "\n",
    "            label_list = []\n",
    "            L_vertex = []\n",
    "            titles = []\n",
    "            indexes = []\n",
    "            graphs = []\n",
    "            path2_table = []\n",
    "            sub2word_list = []\n",
    "            word2sent_list = []\n",
    "\n",
    "            for i, example in enumerate(minibatch):\n",
    "                entities, labels, na_triple, word_id, pos_id, ner_id = (\n",
    "                    example[\"entities\"],\n",
    "                    example[\"labels\"],\n",
    "                    example[\"na_triple\"],\n",
    "                    example[\"word_id\"],\n",
    "                    example[\"pos_id\"],\n",
    "                    example[\"ner_id\"],\n",
    "                )\n",
    "                # graphs.append(dgl.add_self_loop(example[\"graph2\"]).to(\"cuda:0\"))\n",
    "                graphs.append(dgl.add_self_loop(example[\"graph2\"]))\n",
    "                path2_table.append(example[\"path2\"])\n",
    "\n",
    "                prewrong = example.get(\"wrong_predits\", [])\n",
    "\n",
    "                sub2word_list.append(torch.Tensor(example[\"sub2word\"]))\n",
    "                word2sent_list.append(torch.Tensor(example[\"word2sent\"]))\n",
    "                L = len(entities)\n",
    "                word_num = word_id.shape[0]\n",
    "\n",
    "                context_word_ids[i, :word_num].copy_(torch.from_numpy(word_id))\n",
    "                context_pos_ids[i, :word_num].copy_(torch.from_numpy(pos_id))\n",
    "                context_ner_ids[i, :word_num].copy_(torch.from_numpy(ner_id))\n",
    "\n",
    "                idx2label = defaultdict(list)\n",
    "                evid2label = defaultdict(list)\n",
    "                label_set = {}\n",
    "                for label in labels:\n",
    "                    head, tail, relation, intrain = (\n",
    "                        label[\"h\"],\n",
    "                        label[\"t\"],\n",
    "                        label[\"r\"],\n",
    "                        label[\"in_train\"],\n",
    "                    )\n",
    "                    idx2label[(head, tail)].append(relation)\n",
    "                    evid2label[(head, tail)].extend(label[\"evidence\"])\n",
    "                    label_set[(head, tail, relation)] = intrain\n",
    "\n",
    "                label_list.append(label_set)\n",
    "\n",
    "                if self.dataset_type == \"train\":\n",
    "                    train_tripe = list(idx2label.keys())\n",
    "                    na_train_triple = set()\n",
    "                    for j, (h_idx, t_idx) in enumerate(train_tripe):\n",
    "                        ht_pairs[i, j, :] = torch.Tensor([h_idx + 1, t_idx + 1])\n",
    "                        label = idx2label[(h_idx, t_idx)]\n",
    "                        for r in label:\n",
    "                            relation_multi_label[i, j, r] = 1\n",
    "\n",
    "                        relation_mask[i, j] = 1\n",
    "                        relation_label[i, j] = 1\n",
    "                        relation_example_idx[i, j] = example[\"idx\"]\n",
    "\n",
    "                    # =========================================\n",
    "                    # This is for forcing selecting challenging negative pairs\n",
    "                    #     if (t_idx, h_idx) not in idx2label:\n",
    "                    #         na_train_triple.add((t_idx, h_idx))\n",
    "\n",
    "                    to_sample = min(len(train_tripe), int(len(prewrong) * 0.1))\n",
    "                    na_train_triple = random.sample(prewrong, to_sample)\n",
    "                    self.boosted += to_sample\n",
    "                    for j, (h_idx, t_idx) in enumerate(\n",
    "                        na_train_triple, len(train_tripe)\n",
    "                    ):\n",
    "                        ht_pairs[i, j, :] = torch.Tensor([h_idx + 1, t_idx + 1])\n",
    "                        relation_multi_label[i, j, 0] = 1\n",
    "                        relation_label[i, j] = 0\n",
    "                        relation_mask[i, j] = 1\n",
    "                        relation_example_idx[i, j] = example[\"idx\"]\n",
    "                    # =========================================\n",
    "\n",
    "                    lower_bound = len(na_triple)\n",
    "                    if self.negativa_alpha > 0.0:\n",
    "                        random.shuffle(na_triple)\n",
    "                        lower_bound = int(\n",
    "                            max(20, len(train_tripe) * self.negativa_alpha)\n",
    "                        )\n",
    "                    lower_bound -= len(na_train_triple)\n",
    "\n",
    "                    for j, (h_idx, t_idx) in enumerate(\n",
    "                        na_triple[:lower_bound], len(train_tripe) + len(na_train_triple)\n",
    "                    ):\n",
    "                        ht_pairs[i, j, :] = torch.Tensor([h_idx + 1, t_idx + 1])\n",
    "                        relation_multi_label[i, j, 0] = 1\n",
    "                        relation_label[i, j] = 0\n",
    "                        relation_mask[i, j] = 1\n",
    "                        relation_example_idx[i, j] = example[\"idx\"]\n",
    "\n",
    "                    max_h_t_cnt = max(\n",
    "                        max_h_t_cnt,\n",
    "                        len(train_tripe) + lower_bound + len(na_train_triple),\n",
    "                    )\n",
    "                else:\n",
    "                    j = 0\n",
    "                    for h_idx in range(L):\n",
    "                        for t_idx in range(L):\n",
    "                            if h_idx != t_idx:\n",
    "                                ht_pairs[i, j, :] = torch.Tensor([h_idx + 1, t_idx + 1])\n",
    "                                relation_mask[i, j] = 1\n",
    "                                relation_example_idx[i, j] = example[\"idx\"]\n",
    "                                j += 1\n",
    "\n",
    "                    max_h_t_cnt = max(max_h_t_cnt, j)\n",
    "                    L_vertex.append(L)\n",
    "                    # titles.append(example[\"title\"])\n",
    "                    indexes.append(self.batches_order[idx][i])\n",
    "                titles.append(example[\"title\"])\n",
    "\n",
    "            context_word_mask = context_word_ids > 0\n",
    "            context_word_length = context_word_mask.sum(1)\n",
    "            batch_max_length = context_word_length.max()\n",
    "            sub2word_list = [sw[:, :batch_max_length] for sw in sub2word_list]\n",
    "            word2sent_list = [ws for ws in word2sent_list]\n",
    "\n",
    "            # yield {\n",
    "            #     \"context_idxs\": get_cuda(\n",
    "            #         context_word_ids[:cur_bsz, :batch_max_length].contiguous()\n",
    "            #     ),\n",
    "            #     \"context_pos\": get_cuda(\n",
    "            #         context_pos_ids[:cur_bsz, :batch_max_length].contiguous()\n",
    "            #     ),\n",
    "            #     \"context_ner\": get_cuda(\n",
    "            #         context_ner_ids[:cur_bsz, :batch_max_length].contiguous()\n",
    "            #     ),\n",
    "            #     \"context_word_mask\": get_cuda(\n",
    "            #         context_word_mask[:cur_bsz, :batch_max_length].contiguous()\n",
    "            #     ),\n",
    "            #     \"context_word_length\": get_cuda(\n",
    "            #         context_word_length[:cur_bsz].contiguous()\n",
    "            #     ),\n",
    "            #     \"h_t_pairs\": get_cuda(ht_pairs[:cur_bsz, :max_h_t_cnt, :2]),\n",
    "            #     \"relation_label\": get_cuda(\n",
    "            #         relation_label[:cur_bsz, :max_h_t_cnt]\n",
    "            #     ).contiguous(),\n",
    "            #     \"relation_multi_label\": get_cuda(\n",
    "            #         relation_multi_label[:cur_bsz, :max_h_t_cnt]\n",
    "            #     ),\n",
    "            #     \"relation_mask\": get_cuda(relation_mask[:cur_bsz, :max_h_t_cnt]),\n",
    "            #     \"relation_example_idx\": relation_example_idx[:cur_bsz, :max_h_t_cnt],\n",
    "            #     \"labels\": label_list,\n",
    "            #     \"graph2s\": graphs,\n",
    "            #     \"sub2words\": sub2word_list,\n",
    "            #     \"word2sents\": word2sent_list,\n",
    "            #     \"path2_table\": path2_table,\n",
    "            #     \"L_vertex\": L_vertex,\n",
    "            #     \"titles\": titles,\n",
    "            #     \"indexes\": indexes,\n",
    "            # }\n",
    "\n",
    "            yield {\n",
    "                \"context_idxs\": context_word_ids[\n",
    "                    :cur_bsz, :batch_max_length\n",
    "                ].contiguous(),\n",
    "                \"context_pos\": context_pos_ids[\n",
    "                    :cur_bsz, :batch_max_length\n",
    "                ].contiguous(),\n",
    "                \"context_ner\": context_ner_ids[\n",
    "                    :cur_bsz, :batch_max_length\n",
    "                ].contiguous(),\n",
    "                \"context_word_mask\": context_word_mask[\n",
    "                    :cur_bsz, :batch_max_length\n",
    "                ].contiguous(),\n",
    "                \"context_word_length\": context_word_length[:cur_bsz].contiguous(),\n",
    "                \"h_t_pairs\": ht_pairs[:cur_bsz, :max_h_t_cnt, :2],\n",
    "                \"relation_label\": relation_label[:cur_bsz, :max_h_t_cnt].contiguous(),\n",
    "                \"relation_multi_label\": relation_multi_label[:cur_bsz, :max_h_t_cnt],\n",
    "                \"relation_mask\": relation_mask[:cur_bsz, :max_h_t_cnt],\n",
    "                \"relation_example_idx\": relation_example_idx[:cur_bsz, :max_h_t_cnt],\n",
    "                \"labels\": label_list,\n",
    "                \"graph2s\": graphs,\n",
    "                \"sub2words\": sub2word_list,\n",
    "                \"word2sents\": word2sent_list,\n",
    "                \"path2_table\": path2_table,\n",
    "                \"L_vertex\": L_vertex,\n",
    "                \"titles\": titles,\n",
    "                \"indexes\": indexes,\n",
    "            }\n",
    "\n",
    "    def feedback(self, m_preds, m_label, r_mask, h_t_pairs, relation_example_idx):\n",
    "        output_m = torch.argmax(m_preds, dim=-1)\n",
    "        output_m = output_m.data.cpu().numpy()\n",
    "        m_label = m_label.data.cpu().numpy()\n",
    "        r_mask = r_mask.data.cpu().numpy()\n",
    "        h_t_pairs = h_t_pairs.data.cpu().numpy()\n",
    "        wrong_predits = {}\n",
    "        for i in range(len(r_mask)):\n",
    "            for j in range(len(r_mask[0])):\n",
    "                idx = int(relation_example_idx[i, j])\n",
    "                ent_0, ent_1 = h_t_pairs[i, j, 0], h_t_pairs[i, j, 1]\n",
    "                if r_mask[i, j] == 1 and m_label[i, j, output_m[i, j]] == 0:\n",
    "                    if idx not in wrong_predits:\n",
    "                        wrong_predits[idx] = []\n",
    "                    wrong_predits[idx].append((ent_0 - 1, ent_1 - 1))\n",
    "        for idx in wrong_predits:\n",
    "            self.data[idx][\"wrong_predits\"] = wrong_predits.get(idx, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DGLREDataloader(\n",
    "    train_set,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    negativa_alpha=args.negativa_alpha,\n",
    ")\n",
    "\n",
    "dev_loader = DGLREDataloader(\n",
    "    dev_set, batch_size=args.test_batch_size, dataset_type=\"dev\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import nn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, activation=None, dropout=0.0):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.act = activation\n",
    "        self.drop = nn.Dropout(p=dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        X = self.drop(X)\n",
    "        X = torch.matmul(A, X)\n",
    "        X = self.proj(X)\n",
    "        X = self.act(X) if self.act else X\n",
    "        return X\n",
    "\n",
    "\n",
    "class AttnLayer(nn.Module):\n",
    "    def __init__(self, in_feats, activation=None, dropout=0.0):\n",
    "        super(AttnLayer, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(in_feats, 8, dropout=dropout)\n",
    "        self.activation = activation\n",
    "        self.v_proj = nn.Linear(in_feats, in_feats)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        query = query.unsqueeze(1)\n",
    "        key = key.unsqueeze(1)\n",
    "        value = self.v_proj(value)\n",
    "        value = value.unsqueeze(1)\n",
    "        out_fea = self.attn(query, key, value, need_weights=False)[0]\n",
    "        out_fea = out_fea.squeeze(1)\n",
    "        if self.activation:\n",
    "            return self.activation(out_fea)\n",
    "        return out_fea\n",
    "\n",
    "\n",
    "def norm_g(g):\n",
    "    degrees = torch.sum(g, 1)\n",
    "    g = g / degrees\n",
    "    return g\n",
    "\n",
    "\n",
    "def filter_g(g, features):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    sim_A = cos(features.unsqueeze(2), features.t().unsqueeze(0))\n",
    "    # adj = g.adj().to_dense().cuda()\n",
    "    adj = g.adj().to_dense()\n",
    "    unorder = ((adj + adj.t()) == 2).float()\n",
    "    # print('================>', (adj - unorder).sum())\n",
    "    ordered = (adj - unorder) * (sim_A > 0.6)\n",
    "    # print('================>', ordered.sum())\n",
    "    A = ordered + unorder\n",
    "    return norm_g(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class SAGDRE_BERT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SAGDRE_BERT, self).__init__()\n",
    "        self.config = config\n",
    "        self.activation = nn.ReLU()\n",
    "        self.entity_type_emb = nn.Embedding(\n",
    "            config.entity_type_num,\n",
    "            config.entity_type_size,\n",
    "            padding_idx=config.entity_type_pad,\n",
    "        )\n",
    "        self.entity_id_emb = nn.Embedding(\n",
    "            config.max_entity_num + 1,\n",
    "            config.entity_id_size,\n",
    "            padding_idx=config.entity_id_pad,\n",
    "        )\n",
    "\n",
    "        if config.model_name == \"SAGDRE_BERT_base\":\n",
    "            self.bert = BertModel.from_pretrained(\"bert-base-cased\", return_dict=False)\n",
    "        else:\n",
    "            self.bert = BertModel.from_pretrained(\"bert-large-cased\", return_dict=False)\n",
    "\n",
    "        self.start_dim = config.bert_hid_size\n",
    "\n",
    "        # if config.use_entity_type:\n",
    "        self.start_dim += config.entity_type_size + config.entity_id_size\n",
    "\n",
    "        self.gcn_dim = config.gcn_dim\n",
    "\n",
    "        self.start_gcn = GCNLayer(self.start_dim, self.gcn_dim)\n",
    "\n",
    "        self.GCNs = nn.ModuleList(\n",
    "            [\n",
    "                GCNLayer(self.gcn_dim, self.gcn_dim, activation=self.activation)\n",
    "                for i in range(config.gcn_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.Attns = nn.ModuleList(\n",
    "            [\n",
    "                AttnLayer(self.gcn_dim, activation=self.activation)\n",
    "                for _ in range(config.gcn_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.bank_size = self.start_dim + self.gcn_dim * (self.config.gcn_layers + 1)\n",
    "        self.dropout = nn.Dropout(self.config.dropout)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            self.bank_size, self.bank_size, 2, bidirectional=False, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.path_attn = nn.MultiheadAttention(self.bank_size, 4)\n",
    "\n",
    "        self.predict2 = nn.Sequential(\n",
    "            nn.Linear(self.bank_size * 5, self.bank_size * 5),\n",
    "            self.activation,\n",
    "            self.dropout,\n",
    "        )\n",
    "\n",
    "        self.out_linear = nn.Linear(self.bank_size * 5, config.relation_nums)\n",
    "        self.out_linear_binary = nn.Linear(self.bank_size * 5, 2)\n",
    "\n",
    "    def forward(self, **params):\n",
    "        # words = params[\"words\"].cuda()\n",
    "        # mask = params[\"mask\"].cuda()\n",
    "        words = params[\"words\"]\n",
    "        mask = params[\"mask\"]\n",
    "        bsz = words.size(0)\n",
    "\n",
    "        encoder_outputs, sent_cls = self.bert(input_ids=words, attention_mask=mask)\n",
    "        encoder_outputs = torch.cat(\n",
    "            [\n",
    "                encoder_outputs,\n",
    "                self.entity_type_emb(params[\"entity_type\"]),\n",
    "                self.entity_id_emb(params[\"entity_id\"]),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        graphs = params[\"graph2s\"]\n",
    "        sub2words = params[\"sub2words\"]\n",
    "        features = []\n",
    "\n",
    "        for i, graph in enumerate(graphs):\n",
    "            encoder_output = encoder_outputs[i]\n",
    "            sub2word = sub2words[i]\n",
    "            x = torch.mm(sub2word, encoder_output)\n",
    "            graph = filter_g(graph, x)\n",
    "            xs = [x]\n",
    "            x = self.start_gcn(graph, x)\n",
    "            xs.append(x)\n",
    "            for GCN, Attn in zip(self.GCNs, self.Attns):\n",
    "                x1 = GCN(graph, x)\n",
    "                x2 = Attn(x, x1, x1)\n",
    "                x = x1 + x2\n",
    "                xs.append(x)\n",
    "            out_feas = torch.cat(xs, dim=-1)\n",
    "            features.append(out_feas)\n",
    "\n",
    "        h_t_pairs = params[\"h_t_pairs\"]\n",
    "        h_t_pairs = h_t_pairs + (h_t_pairs == 0).long() - 1\n",
    "        h_t_limit = h_t_pairs.size(1)\n",
    "        # path_info = torch.zeros((bsz, h_t_limit, self.bank_size)).cuda()\n",
    "        path_info = torch.zeros((bsz, h_t_limit, self.bank_size))\n",
    "        rel_mask = params[\"relation_mask\"]\n",
    "        path_table = params[\"path2_table\"]\n",
    "\n",
    "        path_len_dict = defaultdict(list)\n",
    "\n",
    "        entity_num = torch.max(params[\"entity_id\"])\n",
    "        # entity_bank = torch.Tensor(bsz, entity_num, self.bank_size).cuda()\n",
    "        entity_bank = torch.Tensor(bsz, entity_num, self.bank_size)\n",
    "\n",
    "        for i in range(len(graphs)):\n",
    "            max_id = torch.max(params[\"entity_id\"][i])\n",
    "            entity_feas = features[i][-max_id:]\n",
    "            entity_bank[i, : entity_feas.size(0)] = entity_feas\n",
    "            path_t = path_table[i]\n",
    "            for j in range(h_t_limit):\n",
    "                h_ent = h_t_pairs[i, j, 0].item()\n",
    "                t_ent = h_t_pairs[i, j, 1].item()\n",
    "\n",
    "                if rel_mask is not None and rel_mask[i, j].item() == 0:\n",
    "                    break\n",
    "\n",
    "                if rel_mask is None and h_ent == 0 and t_ent == 0:\n",
    "                    continue\n",
    "\n",
    "                # path = path_t[(h_ent+1, t_ent+1)]\n",
    "                paths = path_t[(h_ent + 1, t_ent + 1)]\n",
    "                for path in paths:\n",
    "                    # path = torch.LongTensor(path).cuda()\n",
    "                    path = torch.LongTensor(path)\n",
    "                    cur_h = torch.index_select(features[i], 0, path)\n",
    "                    path_len_dict[len(path)].append((i, j, cur_h))\n",
    "\n",
    "        h_ent_idx = h_t_pairs[:, :, 0].unsqueeze(-1).expand(-1, -1, self.bank_size)\n",
    "        t_ent_idx = h_t_pairs[:, :, 1].unsqueeze(-1).expand(-1, -1, self.bank_size)\n",
    "        h_ent_feas = torch.gather(input=entity_bank, dim=1, index=h_ent_idx)\n",
    "        t_ent_feas = torch.gather(input=entity_bank, dim=1, index=t_ent_idx)\n",
    "\n",
    "        path_embedding = {}\n",
    "\n",
    "        for items in path_len_dict.values():\n",
    "            cur_hs = torch.stack([h for _, _, h in items], 0)\n",
    "            cur_hs2, _ = self.rnn(cur_hs)\n",
    "            cur_hs = cur_hs2.max(1)[0]\n",
    "            for idx, (i, j, _) in enumerate(items):\n",
    "                if (i, j) not in path_embedding:\n",
    "                    path_embedding[(i, j)] = []\n",
    "                path_embedding[(i, j)].append(cur_hs[idx])\n",
    "\n",
    "        querys = h_ent_feas - t_ent_feas\n",
    "\n",
    "        for (i, j), emb in path_embedding.items():\n",
    "            query = querys[i : i + 1, j : j + 1]\n",
    "            keys = torch.stack(emb).unsqueeze(1)\n",
    "            output, attn_weights = self.path_attn(query, keys, keys)\n",
    "            path_info[i, j] = output.squeeze(0).squeeze(0)\n",
    "\n",
    "        out_feas = torch.cat(\n",
    "            [\n",
    "                h_ent_feas,\n",
    "                t_ent_feas,\n",
    "                torch.abs(h_ent_feas - t_ent_feas),\n",
    "                torch.mul(h_ent_feas, t_ent_feas),\n",
    "                path_info,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        out_feas = self.predict2(out_feas)\n",
    "        m_preds = self.out_linear(out_feas)\n",
    "        b_preds = self.out_linear_binary(out_feas)\n",
    "        return m_preds, b_preds, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGDRE_BERT(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "from operator import itemgetter\n",
    "from random import shuffle\n",
    "import pickle\n",
    "\n",
    "\n",
    "def search_sample_batch_with_title(title, data_loader):\n",
    "\n",
    "    sample = None\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        t1, t2 = batch[\"titles\"]\n",
    "        if \"Mighty\" in t1 or \"Mighty\" in t2:\n",
    "            sample = batch\n",
    "            break\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def save_dataset_with_index(train_set, dev_set):\n",
    "\n",
    "    train_data = []\n",
    "    dev_data = []\n",
    "\n",
    "    for idx in tqdm(range(len(train_set))):\n",
    "        train_data.append(train_set[idx])\n",
    "        train_data[idx][\"idx\"] = idx\n",
    "\n",
    "    for idx in tqdm(range(len(dev_set))):\n",
    "        dev_data.append(dev_set[idx])\n",
    "        dev_data[idx][\"idx\"] = idx\n",
    "\n",
    "    with open(\n",
    "        \"/home/hwiric/SagDRE-2024-07-05/data/DocRED/train_data.pkl\", \"wb\"\n",
    "    ) as handle:\n",
    "        pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(\n",
    "        \"/home/hwiric/SagDRE-2024-07-05/data/DocRED/dev_data.pkl\", \"wb\"\n",
    "    ) as handle:\n",
    "        pickle.dump(dev_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def shuffle_indicies(data_set, last_index):\n",
    "\n",
    "    shuffled_indices = list(range(len(data_set)))\n",
    "    shuffle(shuffled_indices)\n",
    "\n",
    "    return shuffled_indices[:last_index]\n",
    "\n",
    "\n",
    "def print_children_nodes(edges, id_to_word, id):\n",
    "\n",
    "    children_ids = edges[id]\n",
    "    children_words = itemgetter(*edges[id])(id_to_word)\n",
    "    children = list(zip(children_ids, children_words))\n",
    "\n",
    "    print(f\"({id_to_word[id]})의 자식 노드: {children}\")\n",
    "\n",
    "\n",
    "# entities added\n",
    "def id_to_word(raw_sample):\n",
    "\n",
    "    id_to_word_dict = defaultdict()\n",
    "    id = 0\n",
    "\n",
    "    for sentence in raw_sample[\"sents\"]:\n",
    "        for word in sentence:\n",
    "            id_to_word_dict[id] = word\n",
    "            id += 1\n",
    "\n",
    "    for entity in raw_sample[\"vertexSet\"]:\n",
    "        entity_name = entity[0][\"name\"]\n",
    "        id_to_word_dict[id] = entity_name\n",
    "        id += 1\n",
    "\n",
    "    return id_to_word_dict\n",
    "\n",
    "\n",
    "class SampleEncoder:\n",
    "    def __init__(self, words_id) -> None:\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\", return_dict=False)\n",
    "        self.words_id = torch.unsqueeze(torch.from_numpy(words_id), 0)\n",
    "        self.mask = torch.unsqueeze(torch.from_numpy(words_id > 0), 0)\n",
    "\n",
    "    def encode(self):\n",
    "\n",
    "        output, x = self.bert(input_ids=self.words_id, attention_mask=self.mask)\n",
    "\n",
    "        return output, x\n",
    "\n",
    "\n",
    "def convert_edges_format_dgl2dict(src, dst):\n",
    "\n",
    "    src = src.detach().cpu().numpy()\n",
    "    dst = dst.detach().cpu().numpy()\n",
    "\n",
    "    edges = defaultdict(list)\n",
    "    for element in list(zip(src, dst)):\n",
    "        edges[element[0]].append(element[1])\n",
    "\n",
    "    return edges\n",
    "\n",
    "\n",
    "def raw_data(src_file):\n",
    "\n",
    "    with open(file=src_file, mode=\"r\", encoding=\"utf-8\") as fr:\n",
    "        ori_data = json.load(fr)\n",
    "\n",
    "    return ori_data\n",
    "\n",
    "\n",
    "def subwords_cnt_dist(raw_data):\n",
    "\n",
    "    my_bert = Bert(\"SAGDRE_BERT_base\")\n",
    "    subwords_cnt = []\n",
    "\n",
    "    for sample in raw_data:\n",
    "        words = []\n",
    "        for sentence in sample[\"sents\"]:\n",
    "            for word in sentence:\n",
    "                words.append(word)\n",
    "\n",
    "        bert_token, bert_starts, bert_subwords = my_bert.subword_tokenize_to_ids(words)\n",
    "        subwords_cnt.append(len(bert_subwords))\n",
    "\n",
    "    return subwords_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2024-05-10-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
